<div class="row">
  <!-- <div class="col-lg-12 text-xs-center">
    <h3 class="section-heading">Education</h3>
  </div> -->
  <div class="col-lg-12">
    <div style="padding: 5px 20px">
      I am currently a PhD candidate at the <a href="http://cm.cecs.anu.edu.au/">Computational Media Lab.</a>
      at the Australian National University. I am supervised by <a href="http://users.cecs.anu.edu.au/~xlx/">Prof. Lexing Xie</a>,
      <a href="http://www.ong-home.my/">Cheng Soon Ong</a> and <a href="http://users.cecs.anu.edu.au/~u4534172/">Alexander Mathews</a>.

      My interests are in natural language processing, graph neural networks, and computational social science.
    </div>

    <div style="padding: 5px 20px">
      <h4>Recent News</h4>
      <p>
        [2021-02]  Minjeong, Siqi, and I will present the demo <a href="https://arxiv.org/pdf/2102.01974.pdf"">"AttentionFlow"</a> at WSDM '21. <br/>
      </p>
    </div>

    <div style="padding: 5px 20px">
      <h4>Selected Projects</h4>

      <div class="row" style="padding: 10px 0; border-top: 1px solid #ddd;" >
        <div class="col-sm-4"  style="padding-left:0">
          <!-- <img width="100%" src="img/graphdiff.gif" alt="" /> -->
          <iframe width="100%" src="https://www.youtube.com/embed/lei1VOJbf40" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          <!-- <iframe src="https://www.youtube.com/embed/lei1VOJbf40yarn" width="100%" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe> -->
        </div>
        <div class="col-sm-8" style="padding-left:0">
          <small>
            <b>Transform and Tell</b>

            <br/> We propose an end-to-end model which generates captions for
            images embedded in news articles. News images present two key
            challenges: they rely on real-world knowledge, especially about
            named entities; and they typically have linguistically rich
            captions that include uncommon words. We address the first
            challenge by associating words in the caption with faces and
            objects in the image, via a multi-modal, multi-head attention
            mechanism. We tackle the second challenge with a state-of-the-art
            transformer language model that uses byte-pair-encoding to generate
            captions as a sequence of word parts. Demo is available at <a
            href="https://transform-and-tell.ml">transform-and-tell.ml</a>
          </small>
        </div>
      </div>
    </div>
  </div>
</div>
